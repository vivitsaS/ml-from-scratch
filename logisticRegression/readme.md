Logistic regression is the classification analog of linear regression, i.e., the aim is to solve a true or false (or any pair of outcomes) type of problem instead of finding the best fitting line. We try to fit an S curve (the sigmoid curve) instead of a straight line to the data. The loss/ cost function used is the binary cross entropy loss. Binary Cross-Entropy is commonly used for binary classification tasks, where you're predicting one of two classes (0 or 1). It measures the dissimilarity between the true binary labels and the predicted probabilities. The expressions for the derivative of this cost function happens to be the same as that in the case of linear regression. Only the expressions for y_predicted are different. The rest is exactly the same as linear regresion.
